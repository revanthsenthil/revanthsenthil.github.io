<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>revanth's research</title>
    <link rel="stylesheet" href="styles/style.css">
</head>
<body>
    <header>
        <nav>
            <ul>
                <li><a href="index.html" class="nav-item">home</a></li>
                <li><a href="research.html" class="nav-item">research</a></li>
                <!-- <li><a href="projects.html" class="nav-item">projects</a></li>
                <li><a href="blog.html" class="nav-item">blog</a></li> -->
                <li><a href="resume.html" class="nav-item">resume/cv</a></li>
            </ul>
    
            </nav>
    </header>

    <main>
        <!-- <div class="keyword-container">
            <span class="keyword">Robotics</span>
            <span class="keyword">Perception</span>
            <span class="keyword">Locomotion</span>
            <span class="keyword">UAV/UAS</span>
        </div> -->
        <div class="filters-container">
            <h4>filter keywords</h4>
        <button class="filter-button" data-keyword="pathPlanning">Path Planning</button>
        <button class="filter-button" data-keyword="dataset">Dataset</button>
        <button class="filter-button" data-keyword="uav">UAV/UAS</button>
        <button class="filter-button" data-keyword="mrs">Multi-robot</button>
        <button class="filter-button" data-keyword="biosensors">Biosensors</button>
        <button class="filter-button" data-keyword="agriculture">Agriculture</button>
        <button class="filter-button" data-keyword="inspection">Inspection</button>
        <button class="filter-button" data-keyword="cogWL">Cognitive Workload</button>
        <button class="filter-button" data-keyword="hri">Human-Robot Interaction</button>
        <button class="filter-button" data-keyword="perception">Perception</button>
        <!-- <button class="filter-button" data-keyword="locomotion">Locomotion</button> -->
        <button class="filter-button" data-keyword="gui">GUI</button>
        <button class="filter-button" data-keyword="rl">RL/LfD</button>
        <button class="filter-button" data-keyword="userstud">User Study</button>
        </div>
        

        <section class="papers-section">
            <h2>papers</h2>

        <div class="paper-container">
            <div class="paper" data-keywords="Dataset hri gui">
                <h3>ARTEMIS: AI-Driven Robotic Triage labeling and Emergency Information System</h3>
                <div class="paper-keywords">
                    <span class="keyword_p">Dataset</span>
                    <span class="keyword_p">Human-Robot Interaction</span>
                    <span class="keyword_p">GUI</span>
                    <!-- More keywords as needed -->
                </div>
                <h5><strong>Submitted</strong> to IEEE-IROS 2024</h5>
                <p><strong>Revanth Krishna Senthilkumaran</strong>, Mridu Prashanth, Hrishikesh Viswanath, Sathvika Kotha, Kshitij Tiwari, Aniket Bera</p>
                <p>Mass casualty incidents (MCIs) present major challenges for emergency medical services, overwhelming resources and requiring efficient victim assessment. We introduce ARTEMIS, an AI-driven robotic system using speech and natural language processing, and deep learning for acuity classification, deployed on a mobile quadruped for victim localization and severity assessment. Our simulations and outdoor tests demonstrate that ARTEMIS can achieve triage classification precision exceeding 74% overall and 99% for the most critical cases, significantly outperforming existing deep learning-based systems.</p>
                <a href="https://arxiv.org/abs/2309.08865" target="_blank">Read Paper</a>
                <a href="https://www.youtube.com/watch?v=4FU4FRxNwmY" target="_blank">Watch Video</a>
            </div>
            <div class="paper" data-keywords="pathPlanning uav inspection rl">
                <h3>UPPLIED: UAV Path Planning for Learning from Demonstration</h3>
                <div class="paper-keywords">
                    <span class="keyword_p">Path Planning</span>
                    <span class="keyword_p">UAV/UAS</span>
                    <span class="keyword_p">Inspection</span>
                    <span class="keyword_p">RL/LfD</span>
                    <!-- More keywords as needed -->
                </div>
                <h5><strong>Published</strong> in IEEE-IROS 2023</h5>
                <p>Shyam Sundar Kannan*, Vishnunandan L. N. Venkatesh*, <strong>Revanth Krishna Senthilkumaran</strong>, Byung-Cheol Min</p>
                <small>* - Equal Contribution</small>
                <p>In this paper, we present UPPLIED: a novel path-planning framework for UAV-based visual inspection of large structures. Leveraging demonstrated trajectories, UPPLIED generates new, optimized paths for inspecting similar structures, maintaining geometric consistency and targeted region focus. The effectiveness of our approach is demonstrated through various experiments, showcasing its capability to adapt inspection trajectories across different structures with minimal deviation.</p>
                <a href="https://arxiv.org/abs/2303.04284" target="_blank">Read Paper</a>
                <a href="https://www.youtube.com/watch?v=YqPx-cLkv04&feature=youtu.be" target="_blank">Watch Video</a>
            </div>
            <!-- Example paper - repeat structure for each paper -->
            <div class="paper" data-keywords="dataset mrs biosensors cogWL">
                <h3>MOCAS: A Multimodal Dataset for Objective Cognitive Workload Assessment on Simultaneous Tasks</h3>
                <div class="paper-keywords">
                    <span class="keyword_p">Dataset</span>
                    <span class="keyword_p">Multi-robot</span>
                    <span class="keyword_p">Biosensors</span>
                    <span class="keyword_p">Cognitive Workload</span>
                    <!-- More keywords as needed -->
                </div>
                <h5><strong>Under Review</strong> in IEEE-TAFFC 2022</h5>
                <p>Wonse Jo*, Ruiqi Wang*, Su Sun, <strong>Revanth Krishna Senthilkumaran</strong>, Daniel Foti, Byung-Cheol Min</p>
                <small>* - Equal Contribution</small>
                <p>MOCAS introduces a realistic, multimodal dataset for assessing human cognitive workload (CWL), derived from actual CCTV monitoring tasks, unlike traditional virtual game-based datasets. The study integrates physiological and behavioral data from wearable sensors and a webcam, collected from 21 participants, supplemented by CWL self-assessments and personal background surveys. Its technical validation confirms the dataset's effectiveness in eliciting varied CWL levels, making it a valuable tool for real-world CWL recognition.</p>
                <a href="https://arxiv.org/abs/2210.03065" target="_blank">Read Paper</a>
                <!-- Optional video link -->
                <!-- <a href="link-to-video">Watch Video</a> -->
            </div>
            <div class="paper" data-keywords="mrs gui hri">
                <h3>SMARTmBOT: A ROS2-based low-cost and open-source mobile robot platform</h3>
                <div class="paper-keywords">
                    <span class="keyword_p">Multi-robot</span>
                    <span class="keyword_p">GUI</span>
                    <span class="keyword_p">Human-robot Interaction</span>
                    <!-- More keywords as needed -->
                </div>
                <h5><strong>Submitted</strong> to IEEE-IROS 2022</h5>
                <p>Wonse Jo, Jaeeun Kim, Ruiqi Wang, Jeremy Pan, <strong>Revanth Krishna Senthilkumaran</strong>, Byung-Cheol Min</p>
                <p>The paper presents SMARTmBOT, an affordable, open-source mobile robot platform designed for extensive robotics research and education, built on ROS2 and featuring a low-cost, modular, customizable, and expandable design. With most components 3D-printable and a total cost of about $210, SMARTmBOT is accessible and easy to repair, equipped with a comprehensive sensor suite suitable for various tasks like navigation and obstacle avoidance. Its capabilities are demonstrated through diverse experiments, and all source code for sensor integration, camera streaming, and robot control is available in the GitHub Repository. </p> 
                <a href="https://github.com/SMARTlab-Purdue/SMARTmBOT">GitHub Repository</a>
                <a href="https://arxiv.org/abs/2203.08903">Read Paper</a>
            </div>
        </div>
        </section>
        <hr>
        <section class="presentations-section">
            <h2>presentations</h2>
            <div class="presentation-container">

            <div class="paper" data-keywords="biosensors uav agriculture rl dataset">
                <h3>Data Collection for Mobile Manipulators for Learning from Demonstration</h3>
                <div class="paper-keywords">
                    <span class="keyword_p">Perception</span>
                    <span class="keyword_p">RL/LfD</span>
                    <span class="keyword_p">Dataset</span>
                    <!-- More keywords as needed -->
                </div>
                <h5><strong>Presented</strong> at University of Minnesota Summer URC 2023</h5>
                <p>This research focuses on leveraging Spot's advanced hardware and basic behaviors to develop a robust data collection method for facilitating robot learning through task demonstrations, such as 'putting a chair next to a table.' Utilizing demonstrations, a voxel map is created, encompassing robot state and goal information, fed into a modified Vision-Language Model, Per-Act, which generates 3D colored voxels for training. The dataset, comprising around 25 varied demonstrations with Spot, includes comprehensive data recordings used to train the model, emphasizing the practical application of robotic manipulation and navigation.</p>
                <a href="docs/umn_poster_spot.pdf" target="_blank">View Poster</a>

                <!-- Optional video link -->
                <!-- <a href="link-to-video">Watch Video</a> -->
            </div>

            <div class="paper" data-keywords="biosensors uav agriculture">
                <h3>VF-PLUME: Vertical Farming Plant Localizing UAV with Mass Estimation</h3>
                <div class="paper-keywords">
                    <span class="keyword_p">Biosensors</span>
                    <span class="keyword_p">UAV/UAS</span>
                    <span class="keyword_p">Agriculture</span>
                    <!-- More keywords as needed -->
                </div>
                <h5><strong>Presented</strong> at Purdue Spring URC 2023</h5>
                <p>VF-PLUME addresses the challenge of monitoring in compact, high-yield vertical farms by introducing a singular unmanned aerial vehicle (UAV) equipped to assess plant growth and environmental conditions. Utilizing advanced localization algorithms and environmental sensors, the UAV creates detailed 3D maps capturing crucial data like temperature, humidity, and air quality. This innovative approach streamlines farm monitoring, enabling efficient, data-driven agriculture suitable for supporting growing urban populations.</p>
                <a href="docs/VF-PLUME.pdf" target="_blank">Presentation PDF</a>

                <!-- Optional video link -->
                <!-- <a href="link-to-video">Watch Video</a> -->
            </div>

            <div class="paper" data-keywords="biosensors cogWL mrs hri userstud">
                <h3>A Dynamic Cognitive Workload Allocation Method for Human Robot Interaction</h3>
                <div class="paper-keywords">
                    <span class="keyword_p">Biosensors</span>
                    <span class="keyword_p">Multi-robot</span>
                    <span class="keyword_p">Human-Robot Interaction</span>
                    <span class="keyword_p">Cognitive Workload</span>
                    <span class="keyword_p">User Study</span>
                    <!-- More keywords as needed -->
                </div>
                <h5><strong>Presented</strong> at Purdue Fall URC 2022</h5>
                <p>This explores dynamic allocation and measurement of visual perceptual and cognitive workload in HRI using a novel workload allocation algorithm and an affective prediction algorithm, supplemented by a user study. The study leverages the 'Husformer,' a multi-modal framework with cross-modal transformers, to analyze data from biosensors and behavioral sensors, enhancing our understanding of human states during interaction tasks. The effectiveness of this approach is validated through user experiments, building upon prior studies correlating cognitive workload with changes in GUI complexity and object motion, thereby optimizing task allocation based on individual cognitive load.</p>
                <a href="docs/f22_expo.pdf" target="_blank">View Poster</a>

                <!-- Optional video link -->
                <!-- <a href="link-to-video">Watch Video</a> -->
            </div>

            
            <div class="paper" data-keywords="biosensors cogWL mrs hri userstud">
                <h3>A GUI for Measuring Cognitive Workload Stimulus in Human Robot Interaction</h3>
                <div class="paper-keywords">
                    <span class="keyword_p">Biosensors</span>
                    <span class="keyword_p">Multi-robot</span>
                    <span class="keyword_p">Human-Robot Interaction</span>
                    <span class="keyword_p">Cognitive Workload</span>
                    <span class="keyword_p">User Study</span>
                    <!-- More keywords as needed -->
                </div>
                <h5><strong>Presented</strong> at Purdue Spring URC 2022</h5>
                <p>This research introduces a new GUI-based method to assess cognitive stimulus in Human-Robot Interaction, aimed at understanding user workload and state, replacing outdated tools like Dual N-Back. The study involved 30 participants interacting with SMARTmBOTs and using wearable biosensors, with data integrated into a multi-modal perception model to analyze cognitive loads. Developed using PyQt for ROS2 compatibility, the GUI, designed for this specific experiment, is being made open source for broader application in Human-Robot Interaction research.</p>
                <a href="https://www.youtube.com/watch?v=dA7AgpW2gYk" target="_blank">Watch Talk</a>

                <!-- Optional video link -->
                <!-- <a href="link-to-video">Watch Video</a> -->
            </div>

            <!-- Repeat for each paper -->
        </div>
        </section>
    </main>

    <footer>
        <!-- Same footer as in index.html -->
    </footer>

    <script src="scripts/script.js"></script>
</body>
</html>
